# helper
def print_p(param_name, weight):
    print(param_name, ':\n', weight.shape, weight.dtype, "\n------------")

# 完全相等
def test_f(function_name, aim_output, my_output):
    assert torch.equal(aim_output, my_output) ,"{} not pass".format(function_name)

# 近似相等
def test_f2(function_name, aim_output, my_output):
    assert torch.allclose(aim_output, my_output, atol=1e-6, rtol=1e-6), "{} not pass".format(function_name)
#PARAM:

# GlobalCMVN:
# mean & istd
cmvn_mean = model.global_cmvn.mean
cmvn_istd = model.global_cmvn.istd

print_p("cmvn_mean", cmvn_mean)
print_p("cmvn_istd", cmvn_istd)

# linearSubSampling
ls1_linear_weight = model.preprocessing.out[0].weight
ls1_linear_bias = model.preprocessing.out[0].bias

print_p("ls1_linear_weight", ls1_linear_weight)
print_p("ls1_linear_bias", ls1_linear_bias)

# CNN - 0

network0 = model.backbone.network[0]
# dialated cnn
backbone_0_cnn0_weight = network0.cnn[0].weight
backbone_0_cnn0_bias = network0.cnn[0].bias

print_p("cnn0_weight", backbone_0_cnn0_weight)
print_p("cnn0_bias", backbone_0_cnn0_bias)

# batchnorm
backbone_0_bn_gamma = network0.cnn[1].weight.data
backbone_0_bn_beta = network0.cnn[1].bias.data

backbone_0_bn_mean = network0.cnn[1].running_mean
backbone_0_bn_var = network0.cnn[1].running_var

print_p("bn0_gamma", backbone_0_bn_gamma)
print_p("bn0_beta", backbone_0_bn_beta)
print_p("bn0_mean", backbone_0_bn_mean)
print_p("bn0_var", backbone_0_bn_var)

# cnn
backbone_0_cnn1_weight = network0.cnn[3].weight
backbone_0_cnn1_bias = network0.cnn[3].bias

print_p("cnn2_weight", backbone_0_cnn1_weight)
print_p("cnn2_bias", backbone_0_cnn1_bias)
cmvn_mean :
 torch.Size([20]) torch.float32 
------------
cmvn_istd :
 torch.Size([20]) torch.float32 
------------
ls1_linear_weight :
 torch.Size([20, 20]) torch.float32 
------------
ls1_linear_bias :
 torch.Size([20]) torch.float32 
------------
cnn0_weight :
 torch.Size([20, 1, 3]) torch.float32 
------------
cnn0_bias :
 torch.Size([20]) torch.float32 
------------
bn0_gamma :
 torch.Size([20]) torch.float32 
------------
bn0_beta :
 torch.Size([20]) torch.float32 
------------
bn0_mean :
 torch.Size([20]) torch.float32 
------------
bn0_var :
 torch.Size([20]) torch.float32 
------------
cnn2_weight :
 torch.Size([20, 20, 1]) torch.float32 
------------
cnn2_bias :
 torch.Size([20]) torch.float32 
------------
# Implement


# Global_CMVN
# param: mean, var
# get from training set
def my_cmvn(x, mean, istd):
    
    return (x - mean)*istd
# preprocessing - liearSubSampling
# linear
# param: weights, bias
# trained
def my_linear(x, weights, bias):

    return torch.matmul(x, weights.t()) + bias
# preprocessing -liearSubSampling1
# ReLU
# param: none
def my_relu(x):
    return torch.max(x, torch.tensor(0.0))
# backbone - network0
# dilated_cnn
# param: weights, bias


# 在这个function里，我们padding第三维, 前后各pad个0

def my_padding(x, pad):
    batches, features, frames = x.shape
    padded_frames = frames + pad * 2
    padded_x = torch.zeros((batches, features, padded_frames), dtype=x.dtype)
    padded_x[:,:, pad: pad + frames] = x
    return padded_x

def my_dilate_cnn(x, dilation, kernel_size, weights, bias):
    batches, features, frames = x.shape
    padding = ((kernel_size - 1) * dilation) // 2
    padded_x = my_padding(x, padding)
    _, _, padded_frames = padded_x.shape
    output = torch.zeros((batches, features, frames), dtype=x.dtype)

    for batch in range(batches):
        for feature in range(features):
            for frame in range(frames):
                sum_conv = 0
                for k in range(kernel_size):
                    input_index = frame + k * dilation
                    if 0 <= input_index < padded_frames:
                        sum_conv += padded_x[batch, feature, input_index] * weights[feature, 0, k]
                output[batch, feature, frame] = sum_conv + bias[feature]

    return output
    
# backkbone - 0 
# BatchNorm1d
# param： 
### 训练得到 gamma, beta
### 统计训练数据得到 running_mean, running_var

def my_batch_normalize(x, gamma, beta, running_mean, running_var, eps=1e-5):
    x_normalized = (x - running_mean[None, :, None]) / (torch.sqrt(running_var[None, :, None] + eps))
    out = gamma[None, :, None] * x_normalized + beta[None, :, None]

    return out
def my_cnn(x, weights, bias):
    """
    Manually implements a convolution operation for kernel_size=1 and stride=1,
    without modifying the dimensions of input x.

    Parameters:
        x (Tensor): Input tensor of shape [batch, channels, length].
        weights (Tensor): Weights tensor of shape [channels, channels, kernel_size] with kernel_size=1.
        bias (Tensor): Bias tensor of shape [channels].

    Returns:
        Tensor: Output tensor of shape [batch, channels, length].
    """
    # Extract the number of channels from weights assuming shape [channels, channels, 1]
    channels = weights.shape[0]

    # Since the kernel size is 1, we can treat this as a matrix multiplication across each length.
    # Reshape weights for matrix multiplication: [channels, channels]
    weights_reshaped = weights.squeeze(-1).t()

    # Perform the convolution using a loop over each length index
    # Initialize the output tensor with the same shape as x
    output = torch.empty_like(x)

    # Iterate over each position in the length of the input tensor
    for i in range(x.shape[2]):  # Loop over length dimension
        # x[:, :, i] has shape [batch, channels], and weights_reshaped has shape [channels, channels]
        # Perform matrix multiplication and add bias
        output[:, :, i] = torch.matmul(x[:, :, i], weights_reshaped) + bias

    return output
# TEST

# CMVN
temp = model.global_cmvn(feats)
my_temp = my_cmvn(feats, cmvn_mean, cmvn_istd)
test_f("my_cmvn", temp, my_temp)

# Linear
temp = model.preprocessing.out[0](temp)
my_temp = my_linear(my_temp, ls1_linear_weight, ls1_linear_bias)
test_f("my_linear", temp, my_temp)

# ReLU
temp = model.preprocessing.out[1](temp)
my_temp = my_relu(my_temp)
test_f("my_relu", temp, my_temp)

# ==== TRANSPOSE ====
temp_t = temp.transpose(1,2)
my_temp_t = my_temp.transpose(1,2)
test_f("transpose", temp_t, my_temp_t)

# Dilated CNN
temp_t1 = network0.cnn[0](temp_t)
temp_t2 = my_dilate_cnn(temp_t, dilation = 1, kernel_size = 3, weights = backbone_0_cnn0_weight, bias=backbone_0_cnn0_bias )
test_f2("my_dilated_cnn", temp_t1, temp_t2)

# 消除累计误差
temp_t = temp_t1

# Batch Normalize
temp_t1 = network0.cnn[1](temp_t)
temp_t2 = my_batch_normalize(temp_t, bn0_gamma, bn0_beta, bn0_mean, bn0_var, eps=1e-5)
test_f2("my_batch_normalize", temp_t1, temp_t2)

temp_t = temp_t1

# ReLU
temp_t1 = network0.cnn[2](temp_t)
temp_t2 = my_relu(temp_t)
test_f2("my_relu", temp_t1, temp_t2)

temp_t = temp_t1

# CNN
temp1 = network0.cnn[3](temp_t)
temp2 = my_cnn(temp_t, backbone_0_cnn1_weight, backbone_0_cnn1_bias)
test_f2("my_cnn", temp_t1, temp_t2)

temp_t = temp_t1 